# -*- coding: utf-8 -*-
"""Telecom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A3Dtqb0B_ZIT2vZPCybJ5uXRZ8v9vHE_
"""

from google.colab import drive
drive.mount("/content/gdrive")

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
from pandas.core.indexes.interval import InvalidIndexError
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV
import imblearn
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,f1_score
#read the dataset
data=pd.read_csv("Churn_Modelling.csv")
data

data.info()

data.isnull().sum()

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
data["Surname"]=le.fit_transform(data["Surname"])
data["Geography"]=le.fit_transform(data["Geography"])
data["Gender"]=le.fit_transform(data["Gender"])
data["Tenure"]=le.fit_transform(data["Tenure"])
data["Balance"]=le.fit_transform(data["Balance"])
data["NumOfProducts"]=le.fit_transform(data["NumOfProducts"])
data["HasCrCard"]=le.fit_transform(data["HasCrCard"])
data["IsActiveMember"]=le.fit_transform(data["IsActiveMember"])
data["EstimatedSalary"]=le.fit_transform(data["EstimatedSalary"])
data["Exited"]=le.fit_transform(data["Exited"])
data.head()

x=data.iloc[:,0:13].values
y=data.iloc[:,13:14].values
x

y

#from sklearn.preprocessing import OneHotEncoder
one=OneHotEncoder()
a=one.fit_transform(x[:,6:7]).toarray()
b=one.fit_transform(x[:,7:8]).toarray()
c=one.fit_transform(x[:,8:9]).toarray()
d=one.fit_transform(x[:,9:10]).toarray()
e=one.fit_transform(x[:,10:11]).toarray()
f=one.fit_transform(x[:,11:12]).toarray()
g=one.fit_transform(x[:,12:13]).toarray()
x=np.delete(x,[6,7,8,9,10,11,12],axis=1)
x=np.concatenate((a,b,c,d,e,f,g,x),axis=1)

#from imblearn.over_sampling import SMOTE
smt=SMOTE()
x_resample,y_resample= smt.fit_resample(x,y)
x_resample

y_resample
x.shape,x_resample.shape
y.shape,y_resample.shape

data.describe()

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.distplot(data["Tenure"])
plt.subplot(1,2,2)
sns.distplot(data["CreditScore"])

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.countplot(data["Gender"])
plt.subplot(1,2,2)
sns.countplot(data["EstimatedSalary"])

sns.barplot(x="Exited",y="EstimatedSalary",data=data)

sns.heatmap(data.corr(), annot=True)

sns.pairplot(data=data,markers=["^","v"],palette="interno")

#from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_resample,y_resample,test_size=0.1,random_state=0)
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.fit_transform(x_test)
x_train.shape

def logreg(x_train,x_test,y_train,y_test):
  lr=LogisticRegression(random_state=0)
  lr.fit(x_train,y_train)
  y_lr_tr=lr.predict(x_train)
  print(accuracy_score(y_lr_tr,y_train))
  yPred_lr=lr.predict(x_test)
  print(accuracy_score(yPred_lr,y_test))
  print("***Logistic Regression***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_lr))
  print("Classification Report")
  print(classification_report(y_test,yPred_lr))
  logreg(x_train,x_test,y_train,y_test)

def decisionTree(x_train,x_test,y_train,y_test):
  dtc=DecisionTreeClassifier(criterion="entropy",random_state=0)
  dtc.fit(x_train,y_train)
  y_dt_tr=dtc.predict(x_train)
  print(accuracy_score(y_dt_tr,y_train))
  yPred_dt=dtc.predict(x_test)
  print(accuracy_score(yPred_dt,y_test))
  print("***Desicion Tree***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_dt))
  print("Classification Report")
  print(classification_report(y_test,yPred_dt))
  decisionTree(x_train,x_test,y_train,y_test)

def RandomForest(x_train,x_test,y_train,y_test):
  rf=RandomForestClassifier(criterion="entropy",n_estimators=10,random_state=0)
  rf.fit(x_train,y_train)
  y_rf_tr=rf.predict(x_train)
  print(accuracy_score(y_rf_tr,y_train))
  yPred_rf=rf.predict(x_test)
  print(accuracy_score(yPred_rf,y_test))
  print("***  Random Forest***")
  print("Confusion Matrix")
  print(confusion_matrix(y_test,yPred_rf))
  print("Classification Report")
  print(classification_report(y_test,yPred_rf))
  RandomForest(x_train,y_test,y_train,y_test)

def KNN(x_train,x_test,y_train,y_test):
  knn=KNeighborsClassifier()
  knn.fit(x_train,y_train)
  y_knn_tr=knn.predict(x_train)
  print(accuracy_score(y_knn_tr,y_train))
  yPred_knn=knn.predict(x_test)
  print(accuracy_score(yPred_knn,y_test))
  print("***KNN***")
  print("Confusion Matrix")
  print(confusion_matrix(y_test,yPred_knn))
  print("Classification Report")
  print(classification_report(y_test,yPred_knn))
  KNN (x_train,y_test,y_train,y_test)

def svm(x_train,x_test,y_train,y_test):
  svm=SVC(kernal="linear")
  svm.fit(x_train,y_train)
  y_svm_tr=svm.predict(x_train)
  print(accuracy_score(y_svm_tr,y_train))
  yPred_svm=svm.predict(x_test)
  print(accuracy_score(yPred_svm,y_test))
  print("***Support Vector Machine***")
  print("Confusion Matrix")
  print(confusion_matrix(y_test,yPred_svm))
  print("Classification Report")
  print(classification_report(y_test,yPred_svm))
  svm(x_train,y_test,y_train,y_test)

import keras
from keras.models import Sequential
from keras.layers import Dense
classifier=Sequential()
classifier.add(Dense(units=30,activation='relu',input_dim=40))
classifier.add(Dense(units=30,activation='relu'))
classifier.add(Dense(units=1,activation='sigmoid'))
classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model_history=classifier.fit(x_train,y_train,batch_size=10,validation_split=0.33,epochs=200)

ann_pred=classifier.predict(x_test)
ann_pred=(ann_pred>0.5)
ann_pred
print(accuracy_score(ann_pred,y_test))
print("***ANN Model***")
print("Confusion Matrix")
print(confusion_matrix(y_test,ann_pred))
print("Classification Report")
print(classification_report(y_test,ann_pred))

lr=LogisticRegression(random_state=0)
lr.fit(x_train,y_train)
print("Predicting on random input")
lr_pred_own=lr.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",lr_pred_own)

dtc=DecisionTreeClassifier(criterion="entropy",random_state=0)
dtc.fit(x_train,y_train)
print("Predicting on random input")
dtc_pred_own=dtc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",dtc_pred_own)

rf=RandomForestClassifier(criterion="entropy",n_estimators=10,random_state=0)
rf.fit(x_train,y_train)
print("Predicting on random input")
rf_pred_own=rf.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",rf_pred_own)

svc=SVC(kernal="linear")
svc.fit(x_train,y_train)
print("Predicting on random input")
svc_pred_own=svc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",svc_pred_own)

knn=KNeighborsClassifier()
knn.fit(x_train,y_train)
print("Predicting on random input")
knn_pred_own=knn.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",knn_pred_own)

print("Predicting on random input")
ann_pred_own=svc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
ann_pred=(ann_pred>0.5)
ann_pred
print("output is:",ann_pred_own)

def compareModel(x_train,x_test,y_train,y_test):
    logreg(x_train,x_test,y_train,y_test)
    print('-'*100)
    decisionTree(x_train,x_test,y_train,y_test)
    print('-'*100)
    RandomForest(x_train,x_test,y_train,y_test)
    print('-'*100)
    svm(x_train,x_test,y_train,y_test)
    print('-'*100)
    KNN(x_train,x_test,y_train,y_test)
    print('-'*100)
    compareModel(x_train,x_test,y_train,y_test)

print(accuracy_score(ann_pred,y_test))
print("***ANN Model***")
print("Confusion_Matrix")
print(confusion_matrix(y_test,ann_pred))
print("Classification Report")
print(classification_report(y_test,ann_pred))

y_rf =model.predict(x_train)
print(accuracy_score(y_rf,y_train))
yPred_rfcv=model.predict(x_test)
print(accuracy_score(yPred_rfcv,y_test))
print("***Random Forest after Hyperparameter tuning***")
print("Confusion_Matrix")
print(confusion_matrix(y_test,yPred_rfcv))
print("Classification Report")
print(classification_report(y_test,yPred_rfcv))
print("Predicting on random input")
rfcv_pred_own=knn.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",rfcv_pred_own)

classifier.save("telecom_churn.h5")